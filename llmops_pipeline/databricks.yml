bundle:
  name: llmops_pipeline

resources:
  jobs:
    deploy_to_dev:
      name: deploy_to_dev
      tasks:
        - task_key: promote_to_dev
          notebook_task:
            notebook_path: /Workspace/Users/xxx@databricks.com/.bundle/llmops_pipeline/development/files/src/Promote Model
            base_parameters:
              source_model: ""
              source_model_version: ""
              target_model: ""
            source: WORKSPACE
          job_cluster_key: llm-cluster
        - task_key: IS_RAG
          depends_on:
            - task_key: promote_to_dev
          condition_task:
            op: EQUAL_TO
            left: "{{job.parameters.is_rag}}"
            right: "true"
        - task_key: chain_model
          depends_on:
            - task_key: IS_RAG
              outcome: "true"
          notebook_task:
            notebook_path: /Workspace/Users/xxx@databricks.com/.bundle/llmops_pipeline/development/files/src/Build
              RAG and register to UC
            base_parameters:
              vs_host: https://xxx.xx.azuredatabricks.net
              llma_guard_endpoint_token_scope: creds
              foundation_endpoint_name: meta_llama_v3_1_70b_instruct_endpoint
              model_uc_version: "2"
              host: https://xxx.xx.azuredatabricks.net
              vs_token_secret: pat
              llma_guard_endpoint_token_secret: pat
              model_uc: system.ai.meta_llama_v3_1_70b_instruct
              token_secret: pat
              final_model_name: llmops_dev.model_schema.basic_rag_demo_foundation_model
              llma_guard_endpoint: https://xxx.xx.azuredatabricks.net/serving-endpoints/llamaguard/invocations
              vs_endpoint_name: databricks_docs_vector_search
              token_scope: creds
              embedding_model_name: databricks-gte-large-en
              vs_token_scope: creds
              vs_index_fullname: demo_prep.vector_search_data.databricks_documentation_vs_index
            source: WORKSPACE
          job_cluster_key: llm-cluster
        - task_key: spin_up_endpoint
          depends_on:
            - task_key: chain_model
            - task_key: IS_RAG
              outcome: "false"
          run_if: AT_LEAST_ONE_SUCCESS
          notebook_task:
            notebook_path: /Workspace/Users/xxx@databricks.com/.bundle/llmops_pipeline/development/files/src/Create
              Model Serving Endpoint
            base_parameters:
              tracking_table_schema: ""
              tracking_table_catalog: ""
              endpoint_token_secret: pat
              tracking_table_name: ""
              model_name: llmops_dev.model_schema.basic_rag_demo_foundation_model
              endpoint_token_scope: creds
              endpoint_host: https://xxx.xx.azuredatabricks.net
              endpoint_name: dev_llm_endpoint
            source: WORKSPACE
          job_cluster_key: llm-cluster
        - task_key: evaluate_model
          depends_on:
            - task_key: spin_up_endpoint
          notebook_task:
            notebook_path: /Workspace/Users/xxx@databricks.com/.bundle/llmops_pipeline/development/files/src/Evaluate
              Model
            base_parameters:
              challenger_endpoint_token_secret: pat
              challenger_endpoint_host: https://xxx.xx.azuredatabricks.net
              evaluation_dataset: demo_prep.vector_search_data.eval_set_databricks_documentation
              challenger_endpoint_name: dev_llm_endpoint
              champion_endpoint_token_scope: creds
              challenger_endpoint_token_scope: creds
              champion_endpoint_name: prd_llm_endpoint
              champion_endpoint_host: https://xxx.xx.azuredatabricks.net
              champion_endpoint_token_secret: pat_prod
            source: WORKSPACE
          job_cluster_key: llm-cluster
      job_clusters:
        - job_cluster_key: llm-cluster
          new_cluster:
            spark_version: 15.4.x-cpu-ml-scala2.12
            spark_conf:
              spark.master: local[*, 4]
              spark.databricks.cluster.profile: singleNode
            azure_attributes:
              first_on_demand: 1
              availability: ON_DEMAND_AZURE
              spot_bid_max_price: -1
            node_type_id: Standard_D4ds_v5
            custom_tags:
              ResourceClass: SingleNode
            spark_env_vars:
              PYSPARK_PYTHON: /databricks/python3/bin/python3
            enable_elastic_disk: true
            data_security_mode: SINGLE_USER
            runtime_engine: STANDARD
            num_workers: 0
      queue:
        enabled: true
      parameters:
        - name: is_rag
          default: "true"

    deploy_to_qa:  # Workflow that deploys the LLM Model to the QA Environment
      name: deploy_to_qa
      job_clusters:
        - job_cluster_key: llm-cluster
          new_cluster:
            spark_version: 15.4.x-cpu-ml-scala2.12
            spark_conf:
              spark.master: local[*, 4]
              spark.databricks.cluster.profile: singleNode
            azure_attributes:
              first_on_demand: 1
              availability: ON_DEMAND_AZURE
              spot_bid_max_price: -1
            node_type_id: Standard_E4d_v4
            driver_node_type_id: Standard_E4d_v4
            custom_tags:
              ResourceClass: SingleNode
            enable_elastic_disk: true
            data_security_mode: SINGLE_USER
            runtime_engine: STANDARD
            num_workers: 0
      tasks:
        - task_key: promote_to_qa # Workflow task that promotes the model from DEV to QA
          job_cluster_key: llm-cluster
          notebook_task:
            base_parameters:
              source_model: "system.ai.meta_llama_v3_1_70b_instruct" # Model to be promoted from DEV to QA
              source_model_version: "2" # Version of the Model to be promoted from DEV to QA
              target_model: "" # Name of the Model in QA. Leave it empty is the model is not meant to move to another environment. For example, models in the catalog system.ai, which are meant to be shared
            notebook_path: src/Promote Model.py # Notebook containing the source code for this task
        - task_key: IS_RAG
          depends_on:
            - task_key: promote_to_qa
          condition_task:
            op: EQUAL_TO
            left: "{{job.parameters.is_rag}}"
            right: "true"
        - task_key: chain_model # Workflow task that chains the LLM model in a RAG
          job_cluster_key: llm-cluster
          depends_on:
            - task_key: IS_RAG
              outcome: "true"
          notebook_task:
            base_parameters:
              host: "https://xxx.xx.azuredatabricks.net" # QA environment URL (to be used during REST API calls)
              token_scope: "creds"  # Databricks secret scope storing credentials
              token_secret: "pat" # Databricks secret containing a PAT for the QA environment
              foundation_endpoint_name: "meta_llama_v3_1_70b_instruct_endpoint"  # Name of the model serving endpoint for the foundation model in the QA environment
              model_uc: "system.ai.meta_llama_v3_1_70b_instruct" # Model in UC to be deployed
              model_uc_version: "2" # Model version to be deployed
              embedding_model_name: "databricks-gte-large-en" # Embedding model to be used during context retrieval. This model has to be the same one used while populating the vector search index
              vs_endpoint_name: "databricks_docs_vector_search" # Vector Search Endpoint name for RAG
              vs_index_fullname: "demo_prep.vector_search_data.databricks_documentation_vs_index" # Vector Search Index name for RAG
              vs_host: "https://xxx.xx.azuredatabricks.net" # Environment where the Vector Search Endpoint is hosted
              vs_token_scope: "creds" # Databricks secret scope storing credentials for the environment where the Vector Search Endpoint is hosted
              vs_token_secret: "pat_dev" # Databricks secret containing a PAT for the environment where the Vector Search Endpoint is hosted
              llma_guard_endpoint: "https://xxx.xx.azuredatabricks.net/serving-endpoints/llamaguard/invocations" # Llammaguard endpoint URL for RAG
              llma_guard_endpoint_token_scope: "creds" # Databricks secret scope storing credentials for the environment where the Llammaguard endpoint URL is hosted
              llma_guard_endpoint_token_secret: "pat_dev" # Databricks ssecret containing a PAT for the environment where the Llammaguard endpoint URL is hosted
              final_model_name: "llmops_qa.model_schema.basic_rag_demo_foundation_model" # RAG model name that will be created in the QA catalog
            notebook_path: src/Build RAG and register to UC.py # Notebook containing the source code for this task
        - task_key: spin_up_endpoint # Workflow task that spins up a model serving endpoint for the model created in the previus task
          job_cluster_key: llm-cluster
          depends_on:
            - task_key: chain_model
            - task_key: IS_RAG
              outcome: "false"
          run_if: AT_LEAST_ONE_SUCCESS
          notebook_task:
            base_parameters:
              model_name: "llmops_qa.model_schema.basic_rag_demo_foundation_model" # Name of the model created in the previous task
              endpoint_name: "qa_llm_endpoint" # Name of the model serving endpoint that will be created in the QA environment
              endpoint_host: "https://xxx.xx.azuredatabricks.net" # Model Serving endpoint environment
              endpoint_token_scope: "creds" # Databricks secret scope storing credentials for the environment where the  Model Serving endpoint is hosted
              endpoint_token_secret: "pat" # Databricks secret containing a PAT for the environment where the Model Serving endpoint is hosted
              tracking_table_catalog: "" # Model Serving Endpoint Inference Table Catalog. Leave it empty if you don't wish to create inference tables for this endpoint
              tracking_table_schema: "" # Model Serving Endpoint Inference Table Schema
              tracking_table_name: "" # Model Serving Endpoint Inference Table
            notebook_path: src/Create Model Serving Endpoint.py # Notebook containing the source code for this task
      queue:
        enabled: true
      parameters:
        - name: is_rag
          default: "true"

    deploy_to_prod:  # Workflow that deploys the LLM Model to the PROD Environment
      name: deploy_to_prod
      job_clusters:
        - job_cluster_key: llm-cluster
          new_cluster:
            spark_version: 15.4.x-cpu-ml-scala2.12
            spark_conf:
              spark.master: local[*, 4]
              spark.databricks.cluster.profile: singleNode
            azure_attributes:
              first_on_demand: 1
              availability: ON_DEMAND_AZURE
              spot_bid_max_price: -1
            node_type_id: Standard_E4d_v4
            driver_node_type_id: Standard_E4d_v4
            custom_tags:
              ResourceClass: SingleNode
            enable_elastic_disk: true
            data_security_mode: SINGLE_USER
            runtime_engine: STANDARD
            num_workers: 0
      tasks:
        - task_key: promote_to_prod # Workflow task that promotes the model from QA to PROD
          job_cluster_key: llm-cluster
          notebook_task:
            base_parameters:
              source_model: "system.ai.meta_llama_v3_1_70b_instruct" # Model to be promoted from DEV to PROD
              source_model_version: "2" # Version of the Model to be promoted from DEV to PROD
              target_model: "" # Name of the Model in QA. Leave it empty is the model is not meant to move to another environment. For example, models in the catalog system.ai, which are meant to be shared
            notebook_path: src/Promote Model.py
        - task_key: IS_RAG
          depends_on:
            - task_key: promote_to_prod
          condition_task:
            op: EQUAL_TO
            left: "{{job.parameters.is_rag}}"
            right: "true"
        - task_key: chain_model
          depends_on:
            - task_key: IS_RAG
              outcome: "true"
          notebook_task:
            base_parameters:
              host: "https://xxx.xx.azuredatabricks.net" # PROD environment URL (to be used during REST API calls)
              token_scope: "creds" # Databricks secret scope storing credentials
              token_secret: "pat" # Databricks secret containing a PAT
              foundation_endpoint_name: "meta_llama_v3_1_70b_instruct_endpoint" # Name of the model serving endpoint for the foundation model
              model_uc: "system.ai.meta_llama_v3_1_70b_instruct" # Model in UC to be deployed
              model_uc_version: "2" # Model version to be deployed
              embedding_model_name: "databricks-gte-large-en" # Embedding model to be used during context retrieval. This model has to be the same one used while populating the vector search index
              vs_endpoint_name: "databricks_docs_vector_search" # Vector Search Endpoint name for RAG
              vs_index_fullname: "demo_prep.vector_search_data.databricks_documentation_vs_index" # Vector Search Index name for RAG
              vs_host: "https://xxx.xx.azuredatabricks.net" # Environment where the Vector Search Endpoint is hosted
              vs_token_scope: "creds" # Databricks secret scope storing credentials for the environment where the Vector Search Endpoint is hosted
              vs_token_secret: "pat_dev" # Databricks secret containing a PAT for the environment where the Vector Search Endpoint is hosted
              llma_guard_endpoint: "https://xxx.xx.azuredatabricks.net/serving-endpoints/llamaguard/invocations" # Llammaguard endpoint URL for RAG
              llma_guard_endpoint_token_scope: "creds" # Databricks secret scope storing credentials for the environment where the Llammaguard endpoint URL is hosted
              llma_guard_endpoint_token_secret: "pat_dev" # Databricks ssecret containing a PAT for the environment where the Llammaguard endpoint URL is hosted
              final_model_name: "llmops_prod.model_schema.basic_rag_demo_foundation_model" # RAG model name that will be created in the DEV catalog
            notebook_path: src/Build RAG and register to UC.py # Notebook containing the source code for this task
        - task_key: spin_up_endpoint # Workflow task that spins up a model serving endpoint for the model created in the previus task
          job_cluster_key: llm-cluster
          depends_on:
            - task_key: chain_model
            - task_key: IS_RAG
              outcome: "false"
          run_if: AT_LEAST_ONE_SUCCESS
          notebook_task:
            base_parameters:
              model_name: "llmops_prod.model_schema.basic_rag_demo_foundation_model" # Name of the model created in the previous task
              endpoint_name: "prod_llm_endpoint" # Name of the model serving endpoint that will be created in the PROD environment
              endpoint_host: "https://xxx.xx.azuredatabricks.net" # Model Serving endpoint environment
              endpoint_token_scope: "creds" # Databricks secret scope storing credentials for the environment where the  Model Serving endpoint is hosted
              endpoint_token_secret: "pat" # Databricks secret containing a PAT for the environment where the Model Serving endpoint is hosted
              tracking_table_catalog: "llmops_prod" # Model Serving Endpoint Inference Table Catalog. Leave it empty if you don't wish to create inference tables for this endpoint
              tracking_table_schema: "model_tracking" # Model Serving Endpoint Inference Table Schema
              tracking_table_name: "rag_app_realtime" # Model Serving Endpoint Inference Table
            notebook_path: src/Create Model Serving Endpoint.py # Notebook containing the source code for this task
        - task_key: setup_Lakehouse_moniotoring # Workflow task that creates the lakehouse monitoring to monitor how to model performs overtime
          job_cluster_key: llm-cluster
          depends_on:
            - task_key: spin_up_endpoint
          notebook_task:
            base_parameters:
              inference_processed_table: "llmops_prod.model_tracking.rag_app_realtime_payload_processed" # Table that the lakehouse monitoring will monitor
              lakehouse_monitoring_schema: "llmops_prod.model_tracking" # Schema where the lakehouse monitoring will create its tables
            notebook_path: src/Setup Lakehouse Monitoring.py # Notebook containing the source code for this task
        - task_key: setup_human_evaluation_app # Workflow task that creates the human evaluation app
          job_cluster_key: llm-cluster
          depends_on:
            - task_key: setup_Lakehouse_moniotoring
          notebook_task:
            base_parameters:
              model_name: "llmops_prod.model_schema.basic_rag_demo_foundation_model" # Model name that the evaludation app will use
              host: "https://xxx.xx.azuredatabricks.net" # Environment where the evaluation app will be hosted
            notebook_path: src/Create human evaluation app.py
      queue:
        enabled: true
      parameters:
        - name: is_rag
          default: "true"

    compute_metrics_monitoring_prod: # Workflow that calculate metrics to be used by the lakehouse monitoring
      name: compute_metrics_monitoring_prod
      job_clusters:
        - job_cluster_key: llm-cluster
          new_cluster:
            spark_version: 15.4.x-cpu-ml-scala2.12
            spark_conf:
              spark.master: local[*, 4]
              spark.databricks.cluster.profile: singleNode
            azure_attributes:
              first_on_demand: 1
              availability: ON_DEMAND_AZURE
              spot_bid_max_price: -1
            node_type_id: Standard_E4d_v4
            driver_node_type_id: Standard_E4d_v4
            custom_tags:
              ResourceClass: SingleNode
            enable_elastic_disk: true
            data_security_mode: SINGLE_USER
            runtime_engine: STANDARD
            num_workers: 0
      tasks:
        - task_key: compute_metrics_monitoring # Workflow's task that calculate metrics to be used by the lakehouse monitoring
          job_cluster_key: llm-cluster
          notebook_task:
            base_parameters:
              inference_table_name: "llmops_prod.model_tracking.rag_app_realtime_payload" # Model Serving Endpoint inference table
              inference_processed_table: "llmops_prod.model_tracking.rag_app_realtime_payload_processed" # Table containing the inference data plus metrics
              streaming_checkpoint_dir: "/dbfs/tmp/llmops_prod/rag_app" # Checkpoint directory for the streaming service
            notebook_path: src/Compute Inference Table Metrics.py
          job_cluster_key: llm-cluster

targets:
  development:
    workspace:
      host: https://xxx.xx.azuredatabricks.net/
  qa:
    workspace:
      host: https://xxx.xx.azuredatabricks.net/
  prod:
    workspace:
      host: https://xxx.xx.azuredatabricks.net/
